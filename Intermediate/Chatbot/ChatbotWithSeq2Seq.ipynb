{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKLOp79FXwZCHK5R+PX49R"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp9XLK2FjlM0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9b7879c-3946-4c59-b7ad-cbf5168c7fa0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.8.0\n",
            "<class 'keras.src.legacy.preprocessing.text.Tokenizer'>\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.api.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
        "from keras.api.models import Model\n",
        "from keras.src.legacy.preprocessing.text import Tokenizer\n",
        "from keras.api.preprocessing.sequence import pad_sequences\n",
        "\n",
        "print(keras.__version__)\n",
        "print(Tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.api.layers import Input, LSTM, Dense, Embedding, Attention, Concatenate\n",
        "from keras.api.models import Model\n",
        "from keras.src.legacy.preprocessing.text import Tokenizer\n",
        "from keras.api.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "#Load Cornell movie dataset\n",
        "#Assume questions and answers are a list of strings\n",
        "tokenizer =  Tokenizer(num_words=5000, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(questions + answers)\n",
        "#convert text to sequences\n",
        "question_sequences = tokenizer.texts_to_sequences(questions)\n",
        "answer_sequences = tokenizer.texts_to_sequences(answers)\n",
        "\n",
        "#pad sequences\n",
        "max_len = 20\n",
        "X = pad_sequences(question_sequences, maxlen=max_len, padding='post')\n",
        "y = pad_sequences(answer_sequences, maxlen=max_len, padding='post')\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "#Encoder\n",
        "encoder_inputs = Input(shape=(max_len,))\n",
        "enc_emb = Embedding(input_dim=vocab_size, output_dim=256)(encoder_inputs)\n",
        "encoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
        "\n",
        "#Decoder with Attention\n",
        "decoder_inputs = Input(shape=(max_len,))\n",
        "dec_emb = Embedding(input_dim=vocab_size, output_dim=256)(decoder_inputs)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "#Attention layer\n",
        "attention = Attention(use_scale=True)([decoder_outputs, encoder_outputs])\n",
        "decoder_concat = Concatenate()([decoder_outputs, attention])\n",
        "\n",
        "#Output layer\n",
        "output = Dense(vocab_size, activation='softmax')(decoder_concat)\n",
        "\n",
        "#Define Model\n",
        "model = Model(inputs=[encoder_inputs, decoder_inputs], outputs = output)\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
        "\n",
        "#Training\n",
        "model.fit(\n",
        "    [X, y[:, :-1]],     #Teacher forcing : decoder input is shifted left\n",
        "    y[:,1:],              # Target is next word in sequence\n",
        "    epochs = 30,\n",
        "    batch_size = 64,\n",
        ")\n",
        "\n",
        "#Inference(Prediction Loop)\n",
        "\n",
        "#Define encoder model\n",
        "encoder_model = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
        "\n",
        "#Define decoder model(for step by step prediction)\n",
        "decoder_state_input_h = Input(shape=(256,))\n",
        "decoder_state_input_c = Input(shape=(256,))\n",
        "decoder_outputs, state_h, state_c = decoder_lstm(\n",
        "    dec_emb, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
        ")\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs, state_h,state_c],\n",
        ")\n",
        "\n",
        "#Generate Responses\n",
        "def generate_response(input_text):\n",
        "  #Tokenize input\n",
        "  input_seq = tokenizer.texts_to_sequences([input_text])\n",
        "  input_seq = pad_sequences(input_seq, maxlen=max_len, padding='post')\n",
        "\n",
        "  #Encode input\n",
        "  encoder_output, state_h, state_c = encoder_model.predict(input_seq)\n",
        "\n",
        "  # Start decoding with \"<sos>\" token\n",
        "  target_seq = np.zeros((1,1))\n",
        "  target_seq[0,0] = tokenizer.word_index[\"<sos>\"]\n",
        "\n",
        "  #Generate response word by word\n",
        "  response = []\n",
        "  for _ in range(max_len):\n",
        "    output, state_h, state_c = decoder_model.predict([target_seq] + [state_h,state_c])\n",
        "    predicted_word_index = np.argmax(output[0,-1, :])\n",
        "    if predicted_word_index == tokenizer.word_index[\"<eos>\"]:\n",
        "      break\n",
        "    response.append(tokenizer.index_word[predicted_word_index])\n",
        "    target_seq = np.zeros((1,1))\n",
        "    target_seq[0,0] = predicted_word_index\n",
        "\n",
        "    return \" \".join(response)\n"
      ],
      "metadata": {
        "id": "ZG_0EYITcv_f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}